{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Adversarial Losses of GANs</h1>\n",
    "</center>\n",
    "\n",
    "|    Anish Shah    |    Deshana Desai    |Benjamin Ahlbrand|\n",
    "|:----------------:|:-------------------:|:---------------:|\n",
    "|shah.anish@nyu.edu|deshana.desai@nyu.edu| ba1404@nyu.edu  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h4>Abstract</h4>\n",
    "We study a large class of loss functions that have been used for training Generative Adversarial Networks (GANs) called “adversarial losses”. Besides vanilla GAN, we review the loss functions in $f$-GAN, Maximum Mean Discrepancy (MMD) GAN, Wasserstein GAN and Energy-based GAN. We discuss relevant statistical properties of these distance measures that affect their behaviour and how they are employed in GANs. Further, We perform experiments and create simple visualizations to demonstrate relationships of how these distance measures affect the network's ability to cover all modes / generate better samples by covering fewer modes, lead to vanishing gradients, produce disentangled latent spaces or the variance of the cost values as a function of discriminator outputs. We also review the effectiveness of the distance measures in producing samples using metrics such as visual quality, smooth interpolations, inception score on the LSUN dataset. We perform some of these experiments on smaller synthetic datasets due to hardware and computational time bottlenecks. A natural extension of our study in the measurement of the distance between the distributions of generator model and training data, and separately the distributions of the discriminator model versus the training data distribution, is provided by optimal transport theory (OT). Recently, GANs have been used in conjunction with techniques from OT by framing the problem as one of minimization of the transportation cost of moving one data distribution to another. We review and include some of these techniques in our discussion of distance measures.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List of Notations\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>$D$</td>\n",
    "        <td>The discriminator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\omega$</td>\n",
    "        <td>parameter for our discriminator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$f$</td>\n",
    "        <td>A convex, lower-semicontinuous function satisfying $f(1) = 0$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$f^{*}$</td>\n",
    "        <td>Fenchel conjugate of $f$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$V$</td>\n",
    "        <td>$\\mathcal{X} \\mapsto \\mathbb{R}$, output of discriminator without the activation function</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$g_f$</td>\n",
    "        <td>$\\mathbb{R} \\mapsto \\text{dom}_{f^{*}}$, output activation function which respects the domain $\\text{dom}_{f^{*}}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$G$</td>\n",
    "        <td>The generator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$P$</td>\n",
    "        <td>True or Target distribution</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$Q$</td>\n",
    "        <td>Model or generated distribution</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$p(x)$</td>\n",
    "        <td>probability density function of $P$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$q(x)$</td>\n",
    "        <td>probability density function of $Q$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\theta$</td>\n",
    "        <td>parameter for model distribution or generator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$z$</td>\n",
    "        <td>The latent vector</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathcal{Z}$</td>\n",
    "        <td>The latent space</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathcal{X}$</td>\n",
    "        <td>The samples space</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$L$</td>\n",
    "        <td>loss function of GAN </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Divergence Measures\n",
    "\n",
    "A divergence measure is defined as a function which establishes the similarity between two probability distributions. The divergence need not be symmetric (that is, in general the divergence from $p$ to $q$ is not equal to the divergence from $q$ to $p$), and need not satisfy the triangle inequality \\cite{wiki:xxx}.\n",
    "\n",
    "### f-divergence\n",
    "\n",
    "In statistics and probability theory, an $f$-divergence is a function $D_{f}\\left( P \\parallel Q \\right)$ that measures the difference between two probability distributions $P$ and $Q$ \\cite{csiszar2004information, liese2006divergences}. If $P$ and $Q$ are absolutely continuous distributions with respect to a reference $dx$ on $\\mathcal{X}$ and $p$ and $q$ are its probability density function respectively, then we define the $f$-divergence,\n",
    "\n",
    "\\begin{align} \\label{eq:fdiv}\n",
    "    D_f(P \\parallel Q) = \\int_{\\mathcal{X}} q(x) f \\left( \\frac{p(x)}{q(x)} \\right) dx\n",
    "\\end{align}\n",
    "    \n",
    "\n",
    "where the \\textit{generator function} $f: \\mathbb{R}_{+} \\mapsto \\mathbb{R}$ is a convex, lower-semicontinuous function satisfying\n",
    "$f(1) = 0$. Every convex, lower-semicontinuous function $f$ has a \\textit{convex conjugate} function $f^{*}$ known as \\textit{Fenchel conjugate} \\cite{hiriart2012fundamentals}. The function is defined as  $f^{*}(t) = \\sup\\limits_{u \\in \\text{dom}_{f}} \\{ut -  f(u)\\}$,\n",
    "\n",
    "Using Fenchel Conjugate in (\\ref{eq:fdiv}),\n",
    "\n",
    "\\begin{align*}\n",
    "    D_f(P \\parallel Q) &= \\int_{\\mathcal{X}} q(x) \\sup\\limits_{t \\in \\text{dom}_{f^{*}} } \\left\\{ t \\frac{p(x)}{q(x)} - f^{*}(t) \\right\\}  dx \n",
    "    \\intertext{By Jensen Inequality,}\n",
    "    &\\geq \\sup\\limits_{T \\in \\mathcal{T}} \\left( \\int_{\\mathcal{X}}p(x)T(x)dx - \\int_{\\mathcal{X}}q(x)f^{*}(T(x)) dx \\right) \\\\\n",
    "    &= \\sup\\limits_{T \\in \\mathcal{T}} \\left( \\mathbb{E}_{x \\sim P} \\left[T(X)\\right] - \\mathbb{E}_{x \\sim Q} \\left[f^{*}(T(X))\\right] \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathcal{T}$ is an arbitrary class of function $T : \\mathcal{X} \\mapsto \\mathbb{R}$.\n",
    "The lower bound is tight for $T^{*}(x) = f^{'} \\left( \\frac{p(x)}{q(x)} \\right)$ \\cite{nguyen2010estimating} where $f'$ is the first order derivative of $f$.\n",
    "\n",
    "![KL](output_KL.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adversarial Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
